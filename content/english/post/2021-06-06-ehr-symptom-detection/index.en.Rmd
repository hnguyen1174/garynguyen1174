---
title: "EHR Symptom Detection"
summary: In this project, I use deep learning to classify symptoms in nursing notes (EHR)
author: Gary Nguyen
date: '2021-06-06'
output:
  blogdown::html_page:
    highlight: tango
    toc: true
slug: []
thumbnail: "images/nyc_healthcare.jpeg"
categories:
  - Project
tags:
  - NLP
  - R
  - classification
Description: ''
Tags: []
Categories: []
DisableComments: no
---

Technologies:

* Python (`transformers` and `transformers_interpret`) for BERT model building
* Streamlit (`streamlit`) for application
* R (`tidyverse`, `keras`) for data cleaning, visualization and CNN/RNN model building
* R-Shiny (`shiny`) for visualizing the training progresses and NimbleMiner for training set building

### Introduction

#### Overview

In this project, I investigated whether using deep learning models on a medical records corpus of notes will result in better predictive performance compared to traditional machine learning methods. I will use a collection of deidentified patient’s discharge notes that have been annotated for 13 symptoms: dyspnea (shortness of breath), chest pain, nausea, fatigue, cough, dizziness, weight change, palpitation, confusion, peripheral edema and anorexia.

I compared the performance of two traditional machine learning models (logistic regression and random forest) with multiple NLP deep learning models. I found BERT with two additional layers of CNN and LSTM add significant benefits over the baseline models.

Using a deep learning model to detect symptoms from medical notes can save time and resources during outpatient visits. Using this model, a clinician or nurse can quickly pinpoint which symptoms the patient has or had in the past, and allocate resources more efficiently.

#### Challenges

There are many challenges that are associated with natural language processing in healthcare. Previous studies have shown that the inability to effectively model the subtleties of the medical sublanguage hinder performance. In addition, traditional NLP methods can work with synonyms but not similar terms, so many of them suffer from the curse of dimensionality. In the face of such challenges, advanced methods in natural language processing such as CNN, LSTM and BERT are also gaining popularity. 

### Dataset

Our study is conducted on a dataset of discharge notes for patients with heart diseases. The training dataset has 8,031 notes, and the test dataset has 293 notes. The test set is a gold-standard, human-labelled dataset while the training set is generated by [NimberMiner](https://github.com/hnguyen1174/NimbleMiner). There are eleven symptoms in our datasets. They all have class imbalances: the negative class outnumbers the positive class across symptoms. This adds another layer of difficulty to our task. Some deep learning models that we use, such as BERT, address the issue of class imbalance better than others do. The class imbalance is presented as follows:

|Symptom|Training Set|Test Set|
|:--:|:--:|:--:|
|Dyspnea|39%|49%|
|Chest Pain|30%|30%|
|Nausea|21%|21%|
|Fatigue|18%|23%|
|Cough|16%|21%|
|Dizziness|14%|15%|
|Weight Change|10%|9%|
|Palpitation|8%|5%|
|Confusion|8%|11%|
|Peripheral Edema|7%|11%|
|Anorexia|5%|8%|

Our notes are in the form of free-text. They do not have strict structure and vary in length based on the symptoms of each patient and their severity. The training set and the test set are relatively similar in terms of length: the maximum length is 4,547 words for the training set and 4,273 words for the test set, and the average length is 1,025 for the training set and 1,336 for the test set. 

### Method

#### Baseline Models

For this study, I choose logistic regression and random forest as our two baseline models. Logistic regression and random forests have been used widely as predictive models of choice for text classification. For logistic regression, we cast the notes into a sparse matrices before training the model. For random forest, I also cast the notes in sparse matrices, and then used the “ranger” implementation of random forest in the R language, using twenty decision trees and “impurity” as our importance metrics. Among these two baseline models, random forest has been widely used in natural language processing tasks because it is highly interpretable. 

#### Deep Learning Model Training

Traditional natural language processing methods, such as the baseline models, are limited by their vocabulary dictionary for predictive tasks and require complex processing steps. In addition, simplistic models such as logistic regression cannot effectively recognize the semantic relationships between words and n-grams. We choose to perform text classification using word embeddings and convoluted neural networks (CNN) because such models don't have these restrictions. 
We experiment with four different architectures: (1) a 1-layer embedding and 4-layer CNN; (2) a 1-layer fastText embedding and 4-layer CNN and (3) 1-layer embedding, 4-layer CNN followed by a 1-layer Bidirectional Long-short Term Memory (LSTM) and (4) a BERT layer, 2-layer CNN and 1-layer LSTM. We train binary classifications using these three architectures for all eleven symptoms. These models are trained using the Keras library in R with a Tensorflow backend. 

### Results:

We found that deep learning models, on average, produce better performance when compared to traditional machine learning methods. A full comparison can be found in the table below.

|Symptom|Logistic Regression|Random Forest|4-layer CNN|fastText embeddings + 4-layer CNN|4-layer CNN + Bidirectional LSTM|BERT, 2-layer CNN and LSTM|
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|Dyspnea|0.19|0.9|0.91|0.89|0.91|0.93|
|Chest Pain|0.24|0.79|0.86|0.85|0.86|0.88|
|Nausea|0.19|0.42|0.89|0.83|0.86|0.84|
|Fatigue|0.08|0.86|0.85|0.5|0.83|0.88|
|Cough|0.32|0.79|0.81|0.77|0.82|0.84|
|Dizziness|0.14|0.8|0.85|0.47|0.77|0.9|
|Weight Change|0.14|0.65|0.7|0.63|0.69|0.85|
|Palpitation|0.00|0.56|0.75|0.58|0.62|0.72|
|Confusion|0.21|0.9|0.88|0.3|0.88|0.88|
|Peripheral Edema|0.17|0.69|0.64|0.33|0.59|0.8|
|Anorexia|0.00|0.65|0.77|0.34|0.57|0.8|

Since deep learning models outperform the baseline models, below we present the accuracy and F-score of all eleven symptoms using three different deep learning architectures. 

#### Accuracy

![](ehr_chart.png)
#### F1-Score

![](ehr_chart_f1.png)

### Future Extensions

As EHR is becoming ubiquitous and machine learning algorithms for natural language processing are developing very quickly, there are ample opportunities for improvements. There are more features available in our dataset, such as patients’ ages, time and dates of the discharge notes and so on. We can potentially use these features to impose a temporal structure on our data set to not only predict a symptom, but also predict when it will occur in the future.   

### Understanding Symptoms

#### Streamlit application

I also put together a `streamlit` application that applies the trained BERT model to predict whether a note has any of the five symptoms (dyspnea, chest pain, nausea, fatigue and cough). I will deploy this application when receiving appropriate infrastructure from `streamlit`.

![](demo.gif)

### Artifact

* [Github](https://github.com/hnguyen1174/ehr-symptom-detection)
* [Project report (draft)](https://github.com/hnguyen1174/ehr-symptom-detection/blob/master/reports/Paper%20Draft_V3.pdf)